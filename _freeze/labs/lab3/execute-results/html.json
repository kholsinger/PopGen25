{
  "hash": "8ea579ab03c21466149979e7787706d2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lab 3\ntheme: cosmo\nbibliography: references.bib\nexecute:\n  freeze: auto  # re-render only when source changes\n  eval: true\n  echo: true\n---\n\n\n\n# Lab 3: Genetic divergence estimates\n\nToday we'll keep working with genetic data in `R`. The goal is to get those who did not import data last time to import genomic data, think about how it is structured, and build a few more functions that are useful to take a glance at your own data. As we go along, I will also point out packages/software that could have done what we are doing for us more easily, but which you decide to use is up to you. My goal is to make sure you understand how these methods work, how genetic data is structured, and what you'll need for different analyses.\n\n# Part 1: Calculating $F_{ST}$\n\nIn this first part, we'll continue to work on calculating heterozygosity, and calculate $F_{ST}$ for a VCF split between two populations.\n\n## Importing VCFs:\n\nToday we'll work with a different vcf than last time. In the data folder you should be able to find `muc19.vcf.gz`. This is a subsample of the 1000 genomes project data centered around the MUC19 gene. This dataset includes three different populations (MXL,CEU,CHB) which should capture various degrees of divergence. The goal today is:\n\n1)  Read in VCFs if you have not yet.\n\n2)  Write functions to calculate sample allele frequency.\n\n3)  Describe genetic variation in each of the three populations.\n\n4)  Calculate $d_{XY}$ and $F_{ST}$ between these populations.\n\nFor those who get through all of that, we'll then consider how you can try to obtain sequence information to calculate $dN/dS$ and the kinds of tools you'll need to do that.\n\n## Opening a VCF\n\nGo back to last week's [notes](lab2.qmd) to look at how one can use the `vcfR` package to read in data, and see example code to get a genotype matrix. For convenience, the code is given below, but if you never took a look at this last week - now is the time!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(\"vcfR\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: vcfR\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n   *****       ***   vcfR   ***       *****\n   This is vcfR 1.15.0 \n     browseVignettes('vcfR') # Documentation\n     citation('vcfR') # Citation\n   *****       *****      *****       *****\n```\n\n\n:::\n\n```{.r .cell-code}\nrequire(\"readr\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: readr\n```\n\n\n:::\n\n```{.r .cell-code}\nsamples = read_table(\"data/muc_samples.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  sample_id = col_character(),\n  population = col_character()\n)\n```\n\n\n:::\n\n```{.r .cell-code}\nvcf = read.vcfR(\"data/muc19_subsample.vcf.gz\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nScanning file to determine attributes.\nFile attributes:\n  meta lines: 96\n  header_line: 97\n  variant count: 24489\n  column count: 462\n\nMeta line 96 read in.\nAll meta lines processed.\ngt matrix initialized.\nCharacter matrix gt created.\n  Character matrix gt rows: 24489\n  Character matrix gt cols: 462\n  skip: 0\n  nrows: 24489\n  row_num: 0\n\nProcessed variant 1000\nProcessed variant 2000\nProcessed variant 3000\nProcessed variant 4000\nProcessed variant 5000\nProcessed variant 6000\nProcessed variant 7000\nProcessed variant 8000\nProcessed variant 9000\nProcessed variant 10000\nProcessed variant 11000\nProcessed variant 12000\nProcessed variant 13000\nProcessed variant 14000\nProcessed variant 15000\nProcessed variant 16000\nProcessed variant 17000\nProcessed variant 18000\nProcessed variant 19000\nProcessed variant 20000\nProcessed variant 21000\nProcessed variant 22000\nProcessed variant 23000\nProcessed variant 24000\nProcessed variant: 24489\nAll variants processed\n```\n\n\n:::\n\n```{.r .cell-code}\nGT = extract.gt(vcf,element=\"GT\",as.numeric=TRUE)\n```\n:::\n\n\nAt this point, you should have two data structures inside `R`: one is a `tibble` containing two columns: `sample_id` and `population`. We'll use these later, but for now let's focus on the matrix which includes the genotype data. Again, recall that each row is formatted as:\n\n| ind1 | ind2 | ind3 | ... | indN |\n|------|------|------|-----|------|\n| 0    | 1    | 0    | 1   | 2    |\n\nWhere a 0 stands for an individual homozygous for ancestral allele (`a`), a 1 is a heterozygote (`aA`) and a 2 is a homozygote for derived alleles (`AA`).\n\n## Finding an allele frequency\n\nLast week many people struggled with getting an allele frequency out of this genotype matrix, so let's walk through it systematically. Remember that an allele frequency is just the count of that allele divided by the number of chromosomes examined.\n\n$$\np = \\frac{n_{AA}+1/2n_{Aa}}{n_{AA}+n_{Aa}+n_{aa}}\n$$\n\nWe actually don't even need to work *that* hard. The way the GT matrix is formatted, each entry tells you how many derived alleles each individual carries (0,1 or 2). So, we can actually just sum up the values and divide by the number of entries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_GT = function(site){\n  num_derived = sum(site)\n  total_sites = 2*length(site)\n  return(num_derived/total_sites)\n}\n```\n:::\n\n\nSo, if all we want is to get out $p$ and there are no missing values, the code is nice and simple. But, we need to edit the above function to make sure it works even when there are missing sites (it will crash currently - you can test with the `test_data.vcf.gz` file). Instead of using sites to generate values for `num_derived` and `total_sites`, consider first making a new vector that skips any entries with missing data, say `clean_site`, and use that to calculate the allele frequency.\n\nNow, how do we get out sample heterozygosity? The below code, again, works for data where there are no missing values. Change it so it does work with missing values (should only need to change the `n=` line, if you have fixed the `freq_GT` function to work right).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_het = function(site){\n  n = length(which(!is.na(site)))*2\n  p = freq_GT(site)\n  h = n/(n-1)*2*p*(1-p)\n  return(h)\n}\n```\n:::\n\n\nHow do we actually use these functions along the whole VCF? There are two ways to loop over whole data frame, with one being more intuitive, and the other being fast and efficient. You may have learned about `for` loops if you've programmed before, but they are heinously inefficient in `R`. You will instead want to learn to use `apply` functions (`apply`,`sapply`,`lapply`). `apply` will take a dataframe, a dimension (1-row,2-column,1:2-cell) and a function. Then, it applies that function along each dimension of that dataframe. So, running `apply(GT,1,sample_het)` will return the sample heterozygosity along each row of GT (each site).\n\nLet's do that and vizualize:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n```{.r .cell-code}\nrequire(scales)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: scales\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'scales'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n\n\n:::\n\n```{.r .cell-code}\nhets = apply(GT,1,sample_het)\n\nggplot()+geom_histogram(aes(x=hets),bins=50)+\n          labs(x=\"Heterozygosity\",y=\"Count\")+\n          scale_y_continuous(trans=\"log10\")\n```\n\n::: {.cell-output-display}\n![](lab3_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nIf you don't get a nice looking distribution out of this - raise your hand!\n\n## Calculating divergence between pops\n\nTo calculate divergence between populations, we'll need to be able to split the GT matrix into separate GT matrices for each population. Again, there are multiple ways of doing this, but just so you see how things work explicitly, let's actually make these matrices. Which columns do we actually keep? The `samples` data frame has that information.\n\nFor instance, if we want to see the samples that are in the `CEU` population, we could do the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nceu_inds = samples$sample_id[which(samples$population == \"CEU\")]\ncol_ids = which(colnames(GT) %in% ceu_inds)\nCEU_GT = GT[,col_ids]\n```\n:::\n\n\nNow, given that format, try to write a function that does this for any specified GT, population and sample table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubpop_GT = function(GT,samples,population){\n  ## get the individuals belonging to your population in the `samples` table\n  ## find the columns in GT that match those individuals.\n  sub_GT = GT[,col_ids]\n  return(sub_GT)\n}\n```\n:::\n\n\nIf you wrote that function correctly, the below should return three well formatted GT matrices for each population\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCEU_GT = subpop_GT(GT,samples,\"CEU\")\nMXL_GT = subpop_GT(GT,samples,\"MXL\")\nYRI_GT = subpop_GT(GT,samples,\"YRI\")\n```\n:::\n\n\n## Calculating $d_{XY}$\n\nNow, let's write a function to calculate $d_{XY}$ between each population. Remember that for a biallic locus, $d_{XY}$ is defined as:\n\n$$\nd_{XY} = p_1(1-p_2)+p_2(1-p_1)\n$$\n\nWhere $p_i$ is the frequency of the allele in the $i^th$ population.\n\nSo, let's write a function to calculate it at a single site:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndxy_site = function(p1,p2){\n  dxy = p1*(1-p2)+p2*(1-p1)\n  return(dxy)\n}\n```\n:::\n\n\nNow, we just need to get out these allele frequencies. How do we do that?\n\n::: callout-tip\n## Hint\n\nYou don't need to write any new functions to do this - just *apply* the ones you already have\n:::\n\nNow, let's do this in a more efficient way, so that we can easily calculate $d_{XY}$.\n\n## Populating a data table:\n\nIf you look at your `R` environment, it's getting pretty cluttered in there. Instead of a clear structure, you're starting to have all sorts of variables, some of which you may keep, others you will toss. Let's consolidate: I'll recommend we start to build a new data-frame to keep results, and work from there. This will save you a lot of memory as well, since we won't be making separate copies of the GT matrix for each population, for instance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#The GT row names include the chromosome, the position and the alleles. We can use these to start building a data frame.\nids = gsub(\":[A,T,C,G]*:[A,T,C,G]*\",\"\",rownames(GT))\nchr = gsub(\":.*\",\"\",ids)\npos = as.numeric(gsub(\".*:\",\"\",ids))\nsummary_df = data.frame(chr=chr,pos=pos)\nsummary_df$tot_het = apply(GT,1,sample_het)\n#This identifies the columns for which the samples match. Rather than store a new genotype matrix for each, we can just subset on the go.\nceu_ids= which(colnames(GT) %in% samples$sample_id[which(samples$population == \"CEU\")])\nmxl_ids= which(colnames(GT) %in% samples$sample_id[which(samples$population == \"MXL\")])\nyri_ids= which(colnames(GT) %in% samples$sample_id[which(samples$population == \"YRI\")])\n\n#Heterozygosities\nsummary_df$CEU_het = apply(GT[,ceu_ids],1,sample_het)\nsummary_df$MXL_het = apply(GT[,mxl_ids],1,sample_het)\nsummary_df$YRI_het = apply(GT[,yri_ids],1,sample_het)\n\n#Allele frequencies\nsummary_df$CEU_p = apply(GT[,ceu_ids],1,freq_GT)\nsummary_df$MXL_p = apply(GT[,mxl_ids],1,freq_GT)\nsummary_df$YRI_p = apply(GT[,yri_ids],1,freq_GT)\n\n#And now d_xy, we'll use the precalculated p values\nsummary_df$CEU_MXL_dxy = dxy_site(summary_df$CEU_p,summary_df$MXL_p)\n\nsummary_df$CEU_YRI_dxy = dxy_site(summary_df$CEU_p,summary_df$YRI_p)\n\nsummary_df$YRI_MXL_dxy = dxy_site(summary_df$YRI_p,summary_df$MXL_p)\n```\n:::\n\n\n## Plot the $d_{XY}$ values!\n\nPart of the reason in formatting the data more nicely is it's easier to go back and plot. Let's plot the different $d_{XY}$ values. The below code demonstrates how you can layer lots of different elements using ggplot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndxy_plot = ggplot(summary_df,aes(x=pos))+\n    geom_point(aes(y=CEU_YRI_dxy),col=\"coral\",alpha=0.4)+\n    geom_smooth(aes(y=CEU_YRI_dxy),col=\"coral\")+\n    geom_point(aes(y=YRI_MXL_dxy),col=\"cyan\",alpha=0.4)+\n    geom_smooth(aes(y=YRI_MXL_dxy),col=\"cyan\")+\n    geom_point(aes(y=CEU_MXL_dxy),col=\"chartreuse\",alpha=0.4)+\n    geom_smooth(aes(y=CEU_MXL_dxy),col=\"chartreuse\")+\n    labs(x=\"Position(bp)\",y=\"d_xy\")\n```\n:::\n\n\nThat plot probably looks quite awful - recall you are plotting *every* SNP here. A better idea, often, is to use *windowed* statistics. That will be the bonus task - think of how to calculate/summarize these statistics across a moving window of the genome. It's not as straightforward as you may think!\n\n## $F_{ST}$\n\nNow, let's calculate $F_{ST}$. Remember that the Slatkin estimator can be calculated as:\n\n$$\nF_{ST} = 1- \\frac{f_1p_1q_1+f_2p_2q_2}{(f_1p_1+f_2p_2)(f_1q_1+f_2q_2)}\n$$\n\nWhere $f_i$, $p_i$ are the proportion of samples and allele frequency in pop $i$. We already have everything we need to calculate this in the dataframe, minus the relative frequency of samples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_ceu = length(ceu_ids)\nn_mxl = length(mxl_ids)\nn_yri = length(yri_ids)\nsummary_df$CEU_MXL_fst = sapply(1:dim(summary_df)[1],function(i){\n    f1 = n_ceu/(n_ceu+n_mxl)\n    f2 = 1-f1\n    p1 = summary_df$CEU_p[i]\n    p2 = summary_df$MXL_p[i]\n    num = f1*p1*(1-p1)+f2*p2*(1-p2)\n    denom = (f1*p1+f2*p2)*(f1*(1-p1)+f2*(1-p2)) \n    fst = 1-num/denom\n    return(fst)\n})\n```\n:::\n\n\nThe above works, but it's a lot of code to copy paste for each set of populations, and if there's an error that's found later, you'll have to go back and edit it. Write a function that calculates f_ST for any pair of populations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfst_site = function(p1,p2,n1,n2){\n    f1 = n1/(n1+n2)\n    f2 = 1-f1\n    num = f1*p1*(1-p1)+f2*p2*(1-p2)\n    denom = (f1*p1+f2*p2)*(f1*(1-p1)+f2*(1-p2)) \n    fst = 1-num/denom\n    return(fst)\n}\n```\n:::\n\n\nIn fact, let's write a function that takes a genotype matrix and a sample metadata file, and returns all of these statistics in one go. The below is a *massive* function, but you've built up all of the elements throughout this lab:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprocess_GT = function(GT,samples){\n    ids = gsub(\":[A,T,C,G]*:[A,T,C,G]*\",\"\",rownames(GT))\n    chr = gsub(\":.*\",\"\",ids)\n    pos = as.numeric(gsub(\".*:\",\"\",ids))\n    summary_df = data.frame(chr=chr,pos=pos)\n    summary_df$tot_het = apply(GT,1,sample_het)\n    \n    pops = sort(unique(samples$population))\n    #Get indices for each population\n    idx_pops = lapply(pops,function(x) {\n        which(colnames(GT) %in% samples$sample_id[which(samples$population == x)])\n        })\n    #Name them to make retrieval easier.\n    names(idx_pops) = pops\n    #Check for NAs per site per pop\n    obs_alleles = lapply(1:length(pops),function(x){\n        apply(GT[,idx_pops[[x]]],1,function(x) length(!is.na(x)))\n    })\n    names(obs_alleles) = pops\n    summary_df$obs = obs_alleles\n    # Now, let's get allele frequencies         \n    ps = lapply(1:length(pops),function(x) {\n        apply(GT[,idx_pops[[x]]],1,freq_GT)\n    })\n    names(ps) = pops\n    summary_df$p = ps\n    # Heterozygosities\n    hets = lapply(1:length(pops),function(x) {\n        apply(GT[,idx_pops[[x]]],1,sample_het)\n    })\n    names(hets) = pops\n    summary_df$het = hets\n    #All possible pairs of populations:\n    pop_pairs = combn(pops,2)\n    #Calculate fst for each\n    fst =lapply(1:dim(pop_pairs)[2],function(x) {\n            pop1 = pop_pairs[1,x]\n            pop2 = pop_pairs[2,x]\n            n1 = obs_alleles[[pop1]]\n            n2 = obs_alleles[[pop2]]\n            p1 = ps[[pop1]]\n            p2 = ps[[pop2]]\n            return(fst_site(p1,p2,n1,n2))\n    })\n    #Add names to make comparisons clear\n    names(fst) = sapply(1:dim(pop_pairs)[2],function(x) paste(pop_pairs[,x],collapse=\"-\"))\n    #And dxy\n    dxy =lapply(1:dim(pop_pairs)[2],function(x){\n        p1 = ps[[pop_pairs[1,x]]]\n        p2 = ps[[pop_pairs[2,x]]]\n        return(dxy_site(p1,p2))\n    })\n    #Adding names to dxy as well\n    names(dxy) = sapply(1:dim(pop_pairs)[2],function(x) paste(pop_pairs[,x],collapse=\"-\"))\n    #Don't forget to add these to the dataframe\n    summary_df$fst = fst\n    summary_df$dxy = dxy\n    return(summary_df)\n}\n```\n:::\n\n\nIt's a handful to look at the above, but take a look at any of the blocks of calculations. They use `lapply` to loop over the populations (which conveniently returns lists, which can have varying dimensions and so each \"column\" of the dataframe becomes a list of statistics for each population/pair of populations. Within each lapply, functions are called either as `apply` functions over the original `GT`, or get values from previously computed lists. This means that all of your results live in one big dataframe, which includes labels for the different statistics. Analysis becomes fairly straightforward:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot F_ST versus d_XY for a CEU vs MXL\nsummary_df = process_GT(GT,samples)\nggplot(summary_df)+\n    geom_point(aes(x=fst[[\"CEU-MXL\"]],y=dxy[[\"CEU-MXL\"]]))+\nlabs(x=\"F_st\",y=\"d_xy\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 16947 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab3_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n# Part 2: Using `PaML`\n\nFrankly, you likely won't get this far in the lab, and that's ok! I figured refocusing on more VCF manipulation and calculation of pop-gen stats made more sense. But, for completeness of the material we've talked about, I will still guide you through how you would, in principle, calculate dN/dS.\n\n## Case I: All relevant samples in one VCF\n\nLife is simplest if you have a single `VCF` file with all of the samples you'd like to calculate $d_N/d_S$ between. That can be the case if some of your species don't have a good reference genome, so you've mapped a bunch of short reads from different groups to a single annotated reference genome. Annotation is important: we'll need to know where the coding sequence is, and what the open reading frames are.\n\n### Step 1: Generate `fasta` files from VCF.\n\nThis is the simplest case, and as you'll see there's still many steps/tools to learn. For any gene of your interest, we'll need to find the location of its coding sequence in the annotation file (usually a `.gff` file). You can find these files wherever you find a reference genome, most frequently NCBI or RefSeq. GFF files are tab separated files with the format:\n\n\n::: {.cell}\n\n```{.gff .cell-code}\nX\tEnsembl\tRepeat\t2419108\t2419128\t42\t.\t.\thid=trf; hstart=1; hend=21\nX\tEnsembl\tRepeat\t2419108\t2419410\t2502\t-\t.\thid=AluSx; hstart=1; hend=303\nX\tEnsembl\tRepeat\t2419108\t2419128\t0\t.\t.\thid=dust; hstart=2419108; hend=2419128\nX\tEnsembl\tPred.trans.\t2416676\t2418760\t450.19\t-\t2\tgenscan=GENSCAN00000019335\nX\tEnsembl\tGene\t2413425\t2413425\t.\t+\t.\t\nX\tEnsembl\tCDS\t2413805\t2413805\t.\t+\t.\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX\tEnsembl\tRepeat\t2419108\t2419128\t42\t.\t.\thid=trf; hstart=1; hend=21\nX\tEnsembl\tRepeat\t2419108\t2419410\t2502\t-\t.\thid=AluSx; hstart=1; hend=303\nX\tEnsembl\tRepeat\t2419108\t2419128\t0\t.\t.\thid=dust; hstart=2419108; hend=2419128\nX\tEnsembl\tPred.trans.\t2416676\t2418760\t450.19\t-\t2\tgenscan=GENSCAN00000019335\nX\tEnsembl\tGene\t2413425\t2413425\t.\t+\t.\t\nX\tEnsembl\tCDS\t2413805\t2413805\t.\t+\t.\n```\n\n\n:::\n:::\n\n\nThe first column is the chromosome, the second is the source of the annotation. The third is the feature: here, we'll look for a coding sequence (CDS). Then, columns 4 and five give start and stop, respectively. Then a score, a strand, a phase, and finally the important part for searching - the attributes. We need to find our gene of interest, but include all of the regions including its coding sequence. For that, the best tool is a combination of command line utilities, here `awk` and `grep`\n\n#### 1.1 Grep the .gff, awk the grep, bcftools the awk\n\n`grep` is a utility that finds a pattern in a file. It has a surprisingly [fun history](https://www.youtube.com/watch?v=iQZ81MbjKpU).\n\nIt is run as follows:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngrep \"pattern\" file.txt\n```\n:::\n\n\nWhere `pattern` is whatever string you are looking for. `grep` actually uses regular expressions, but there's no way we'll have time to talk about those today. Suffice to say - you'll need to learn about them at some point, so [study up](https://librarycarpentry.github.io/lc-data-intro/01-regular-expressions.html). For today, we won't need to go quite that far.\n\nLet's say we are looking for MUC19 in the human reference genome .gff provided in the data folder (human.gff).\n\nWe can simply run:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngrep \"MUC19\" chr12.gff > muc19_hits.gff\n```\n:::\n\n\nThis will make a new file which only has locations of sequences that included \"MUC19\" somewhere. Now, we can concatenate all of the CDS locations with `awk`, which is a great tool to extract data from tab separated files. The below command, for instance, checks if something is a coding sequence, and if so outputs the location in a format that will let us extract data from the vcf.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nawk '$3==\"CDS\"{print $4 \"-\" $5}' muc19_hits.gff > sequences_to_extract.txt\n```\n:::\n\n\nFinally, we can subset our vcf to *only* include the CDS regions:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nbcftools view -R sequences_to_extract.txt muc19_samples.vcf.gz -Oz -o muc19_cds_only.vcf.gz\n```\n:::\n\n\nNow we have the necessary bits to make a `.fasta` file that includes sequences for all of the samples in your vcf. There are a few ways to do this, but the one you already have all the tools for if you are working with `vcf` files is probably `bcftools consensus`. This command makes `bcftools` emit a fasta with the consensus for the sample named \\<sample\\>. You can make this programattic and loop over all samples, or just run for those you are using for $dN/dS$ calculations.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nbcftools consensus -s <sample> -f reference.fasta muc19_cds_only.vcf.gz > sample.fasta\n```\n:::\n\n\nYou can then just concatenate all of your samples into a single fasta for analysis:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncat sample1.fasta sample2.fasta sample3.fasta > all_samples.fasta\n```\n:::\n\n\n### Step 2 Make a tree!\n\nWe next need to make a tree using the data to guide PaML analysis. [iqtree2](https://iqtree.github.io/) is very fast for this.\n\nThe command to make a tree is also very easy once you have it installed:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\niqtree2 -s all_samples.fasta\n```\n:::\n\n\nThat's it! It will run, and you should have a file called `all_samples.fasta.treefile` containing the maximum likelihood phylogeny for your samples.\n\n### Step 3 Run PaML\n\nWe'll actually run `CodeML`, a subpackage of `PaML` that deals with coding sequence evolution. To get it running, we need the sequences (check), a tree (check) and a control file. Control files for `CodeML` are annoying to generate, and require specifying lots of little details [see here for a guide](https://academic.oup.com/mbe/article/40/4/msad041/7140562). For our purposes, what you'd need is a control file (call it something like `codeml.ctl` that looks like:\n\n\n::: {.cell}\n\n```{.codeml .cell-code}\nseqfile = all_samples.fasta\ntreefile = all_samples.fasta.treefile\noutfile = codeml_1.txt\n\nnoisy = 0\nverbose = 0\n\nseqtype = 1\nndata = 1\nicode = 0\ncleandata = 0\n\nmodel = 0\nNSsites = 0\nCodonFreq = 0\nestFreq = 0\nclock = 0\nfix_omega = 0\nomega = 0.5\n\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nseqfile = all_samples.fasta\ntreefile = all_samples.fasta.treefile\noutfile = codeml_1.txt\n\nnoisy = 0\nverbose = 0\n\nseqtype = 1\nndata = 1\nicode = 0\ncleandata = 0\n\nmodel = 0\nNSsites = 0\nCodonFreq = 0\nestFreq = 0\nclock = 0\nfix_omega = 0\nomega = 0.5\n```\n\n\n:::\n:::\n\n\nI think by this point you are hopefully starting to understand why I'm having you write your own code - every piece of software has its own format requirements, its own assumptions about the data, etc. So, it's better to learn *how* the method runs in general rather than learning a particular software package. You could spend years becoming a `PaML` wizard, but once a new method comes around and all of your old control files no longer work and you don't actually know why.\n\nNow that you have a control file, install `PaML` ([from here](https://github.com/abacus-gene/paml)) and now just run:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ncodeml codeml.ctl\n```\n:::\n\n\nOnce it's done, you should have a data table that contains dN/dS for your sequence of interest.\n\n## Case II: different species, no common VCF\n\nMore commonly, you might find a cool gene in your organism (for thematic sake, let's say it's MUC19), and so you'll want to calculate $d_N/d_S$ by comparing to some related species (let's say Chimpanzee vs human in this case). How do we actually do this? First, we need to get both the amino acid and nucleotide sequences for the gene for both species. This has become vastly simpler than it used to be. Head on over to `NCBI`, search for your gene and you'll find its page:\n\n![](images/paste-3.png)\n\nNow: hit \"Orthologs\", enter the species you'd like to include in your final dataset (let's say narrow to Hominidae first), select species of interest, and then hit the \"Download\" button:\n\n![](images/paste-4.png)\n\nDownload a Package, and make sure it includes only the Protein sequence, and the CDS. Now, I'd recommend including more than one species (maybe all of the primates instead of just a few hominids), but it will make dealing with the data a little harder.\n\n### Why bother with more than one species to compare to?\n\nThe power of PaML comes from using the phylogeny to try and fit a model of evolution for your gene. You can just use a pair of species (`yn00` in `PaML` does just that), but you'll have to do most of the same steps to get there, so taking the extra time to add more species. Whatever you choose to do: unpack the resulting zip, and find the cds.dna and the protein.faa files.\n\n## Aligning proteins\n\nWe now need to align the protein sequences - we don't know what the homologous positions are. This is easiest with something like `Mafft`, although there are many proponents of `muscle`. Note, `muscle5` won't run on `muc19` data without \\~200Gb of RAM, so definitely a task for the cluster. `Mafft` will run just fine, but it may generate a non-sense alignment for some diverged sequences.\n\nAligning is simple:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmafft protein.faa > protein_aligned.faa\n```\n:::\n\n\nBut now we need to align the nucleotide data based on the protein data. Again, a new tool with new syntax: `pal2nal` [found here](https://www.bork.embl.de/pal2nal/).\n\n\n::: {.cell}\n\n```{.bash .cell-code}\npal2nal.pl protein_aligned.faa CDS.fna -output fasta -nogap\n```\n:::\n\n\nOk, we are almost there! Now go back to \\[2 Make a tree!\\] and follow from there using the output fasta you obtained here.\n\nYou might be thinking - hold on, I thought this was harder than the vcf route, and I barely had to run `awk` or `grep` or anything, really. Well, if you're lucky - this *will* be easy and quick. In all likelihood, each piece of software will have *some* issue and you will have to sort out why it's not working. The previous pipeline has the benefit or relying really only on a single piece of software outside of standard unix tools: `bcftools`. Here, we've used NCBIs data servers, `mafft` and `pal2nal` before we got to a file we can build a tree from. Common issues:\n\n-   mafft renamed your protein samples which included a forbidden character like \"-\", but the nucleotide fasta still contains it. Now sequences don't match. (This is an easy enough fix with another standard tool `sed`).\n\n-   PaML does not like your sequence files because they are not a multiple of 3. This shouldn't happen, but it does, and I can't give you a great recommendation of how to fix it. Your downloaded CDS sequences are probably wrong in some way, or your alignment was *really* bad. Try a more stringent alignment approach.\n\n-   pal2nal.pl thinks your amino acids and sequences have different lengths. Sometimes they do, and it's the same issue as above. Other times, your sequence name has a character *it* doesn't like, even though mafft was fine with it (but pal2nal does not say what these are - just keep your sample names simple is all I can say).\n\n-   The gremlins in your computer are tired and won't let the code work. This is common in today's productivity driven environment, and the gremlins get tired all the time. Restart your machine and hope for the best, or try again on the cluster/different computer/different day.\n\n    If all of this sounds like today's lab was going to be a bit of an exercise in frustration - correct! I think we often tend to gloss over just how much work can go into running one simple analysis like $d_N/d_S$, and the familiarity with coding will always make that process infinitely faster than it will be for beginner bioinformaticians. BUT - it's all doable, even if it takes more elbow grease than you would think is worth. More importantly, once you've run it once (and ideally kept good notes), you can probably re-run the analysis for a new set of data relatively quickly - just change the name of a gene in your pipeline and you should be able to re-run it for a different set of species quickly.\n\n# Takeaways\n\nThere are fairly simple ways to calculate divergence when you have a `VCF` file and labels for your populations, even not relying on existing libraries and packages. Understanding how to manipulate genetic data will, in the long run, be much more important for you than knowing how to use, say `PopGenome`. More importantly, make sure you try to write code in a way you can use one statistic calculation (say, allele frequency) in multiple places across the code, rather than re-writing the same calculation over and over. This will not only save you run time, but it will make giant messy pieces of code much easier to deal with.\n\nThere *are* great amazing packages to run all sorts of analyses. Each will want a different set of input files, which often means adding 4-5 more pieces of software to go from the format you have to the one you want. Your arsenal of bioinformatic tools will grow, but keep in mind that understanding base UNIX tools (like `grep`, `awk`, `sed`) is going to help you more in the long run than understanding exactly how a `CodeML` control file is structured and what every line means.\n",
    "supporting": [
      "lab3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}