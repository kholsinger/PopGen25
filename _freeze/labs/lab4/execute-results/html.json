{
  "hash": "2dac30d4c516fb1fb0d4c5ea6b581598",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lab 4\ntheme: cosmo\nbibliography: references.bib\nexecute:\n  freeze: auto  # re-render only when source changes\n  eval: false\n  echo: true\n---\n\n\n\n\n# Goals for today\n\nToday the lab will focus on getting you more acquinted with LD statistics, from calculating and plotting them to LD-pruning. Before that, however, we are now at the point where your code should be getting complex enough that you are probably ready to appreciate a good structure to your codebase. So we'll start with setting up something I recommended a while ago: a file to hold all of your custom functions.\n\n# Part 1: Setting up reusable code re-usably\n\nLet's start by making a file in the popgen github project. Go into the `src` folder and create a file called `<your_last_name>.R` this will be a file that I won't ever overwrite when you download code, but will be a clear space for yourself to store different useful functions as you write them. It's a good idea to generally organize your code even more granularly (for instance, a different file for functions that are reading in/dealing with VCFs than one that works on phylogenetic trees), but for now I'll enforce only a single file. You _will_ be turning these files in, so make sure you actually create it.\n\n## How to structure custom script files\n\nWithin this file, it's probably useful to set up some basic structure as well. First, start by saying what the file contains in a comment (line starting with `#`), then add `require()` statements for any packages the code might depend on. Following this, start adding the code for different functions you've written - but make sure to annotate them.\n\nFor instance, here is what the head of a script file from one of my own projects looks like:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This file contains functions to cluster a network of genes into communities, plot the network using the community data and then analyze each community using GOst\nrequire(gost)\nrequire(igraph)\nrequire(ggplot2)\ntheme_set(theme_void())\n\n#This function takes an igraph network and returns labels for each node based on a fastgreedycluster. If a community contains fewer than `cutoff` members, it is returned as NA instead. This is because downstream analyses are not run on the small communities, which in practice are also often pairs of genes when dealing with ERC scores. \n\nquick_cluster = function(graph){\n...\n}\n```\n:::\n\n\n\nThere are two reasons to annotate code. One, to say what a function does, so that you remember what it is if the name is not great (and in this case, `quick_cluster` is not great). Two, you should add comments to your code to say _why_ you did something that way. Notice my comments spell out I'll be tossing small communities from later analyses, which is why I don't return those. \n\nIn your case - take the time now to create this file, including the functions we used last time to calculate divergence in a VCF. In particular, feel free to copy the `process_GT` function, but make sure you also include all of the functions it relies on (`freq_GT`, `sample_het`,`dxy_site`,`fst_site`, etc.). Now, when you restart `R` in some project and want to be able to run that function, all you should need to do  is run:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd(\"<location of this github repository on your computer>\")\nsource(\"src/<your_last_name.R\")\n```\n:::\n\n\n\nAnd suddenly you'll be able to get all of the stats out of any vcf file that you have population annotations for. For the next bit of the class, we'll work on adding more functionality to this code, this week dealing with LD.\n\n# Part 2: Calculating LD statistics\n\nWe talked in class about a broad set of LD statistics that are often used in different contexts. Some of them will be extremely easy to calculate, while others less so. We'll start with `D` - the absolute measure of linkage disequilbrium. You have most of the bits necessary to write this function already, but we'll need to learn to deal with one more to actually get it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_ij = function(site_i,site_j){\n    p_i = freq_GT(site_i)\n    p_j = freq_GT(site_j)\n    p_ij = ???\n    D_ij = p_ij - p_i*p_j\n    return(D_ij)\n}\n```\n:::\n\n\n\nWhat is `p_ij`? It's the frequency of haplotypes carrying the derived allele at both loci `i` and `j` (e.g. `AB` haplotype instead of `Ab`,`aB`, or `ab`). If you are dealing with haploid data (and I'll assume you have no missing data for the moment), this is easy enough:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_ij = intersect(which(site_i==1),which(site_j==1))/length(site_i)\n```\n:::\n\n\n\nBut, you are often going to deal with diploid data. In that case, when both individuals are coded `2`, you know they have two copies of `AB` haplotypes, but what about a 1?\n\nHere, you need to make sure your data is _phased_ - each allele is essentially labeled whether it came from mom or dad (altough in practice you don't know which is which in many phasing approaches). In your VCF file, phased data looks like `1|0` instead of `1/0`, with the `|` indicating known phase, and `/` indicating it is not known. The 1k genome data you'll work on today happens to be phased, and I'll show you how you can extract that information in a second. But first, let's consider what to do if your data is not phased, but you _really_ need it to be:\n\n## Phasing 101\n\nThe most correct approach to phasing is to have data from a trio of both parents and an offspring. This lets you infer phase directly: if an individual is `0/1` and the mother is `0/0`, while the father is `1/1`, you _know_ the father's allele had to be the `1`. Even here, there'll be ambiguous cases (all three heterozygous, for example), but you should be able to phase most of the genome. This is especially useful when you need accurate phasing but some of the assumptions of statistical phasing might not work (e.g. you are interested in sex chromosomes)\n\nIf you (reasonably) can't triple your sequencing costs for unclear benefits, but still need phasing, there are statistical phasing approaches. `shapeit` has, in my experience, the fastest/most reliable output, but each of the methods has its own assumptions/approaches, so if you are going down this path - read up on them: [Beagle](https://faculty.washington.edu/browning/beagle/beagle.html), [shapeit](https://jmarchini.org/shapeit3/), [fastPhase](https://bioinformaticshome.com/tools/imputation/descriptions/fastPHASE.html#gsc.tab=0).\n\nThe short version of the approaches - the data is generally clustered in a way that best represents multiple populations in Hardy-Weinberg. Then each allele gets assigned a phase based on which of two parental populations (different for each individual, potentially) that individual's alleles seem to come from. These approaches have gotten quite fast and efficient, but even so expect to budget a good month of effort for your project in getting the data phased to a satisfactory outcome.\n\n## Back to calculating LD\n\nSo, what about our phased data once it's in `R`? Notice that in the past I have simply quickly converted the `VCF` to a genotype matrix. But you could instead convert it to a haplotype matrix, with the `vcfR` function `extract.haps`. This function takes the vcf as an argument, and will spit out a nice matrix where each column is one haplotype. By default, it will convert any unphased sites to `NA`s. That's probably the right call in 99% of the cases, but make sure your functions actually work with NAs when dealing with haplotypes. Problematically for us - it returns the actual alleles at each site, rather than the numeric data you have been working with. So, we need to write a wrapper around it that returns a numeric matrix, like our GT matrices so far:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Function to run on each row\nallele_2_num = function(site,name){\n  div_allele = gsub(\".*:.*:([A,T,C,G]+):\",\"\",name)\n  GT_row = as.numeric(site==div_allele)\n  return(GT_row)\n}\n\n#Structure for whole vcf\nextract_haps = function(vcf){\n  haps = extract.haps(vcf)\n  names = rownames(haps)\n  num_mat = t(sapply(1:dim(haps)[1],function(x) allele_2_num(haps[x,],names[x])))\n  rownames(num_mat) = names\n  colnames(num_mat) = colnames(haps)\n  return(num_mat)\n}\n```\n:::\n\n\n\nSo, now the function I showed above for haploids should work for your diploid data, but let's make sure it also works for cases where there are NAs. This isn't immediately clear, but it's easy to trip up here. Imagine you have the genotypes:\n\n| Site | Ind1 | Ind2 | Ind3 | Ind4 | Ind5 | Ind6 |\n|------|------|------|------|------|------|------|\n|   i  |   0  |  NA  |   1  |  0   |   1  |   1  |\n|   j  |   0  |   1  |   0  |  NA  |   0  |   1  |\n|   k  |   NA |  NA  |   1  |  NA  |   1  |   1  |\n\nIf what you do is just `na.omit` each site, you will end up with the wrong LD values for i vs j comparisons, and site 3 is liable to break completely. Each vector would end up looking like:\n\n| Site |    1 |    2 |    3 |    4 |    5 |    6 |\n|------|------|------|------|------|------|------|\n|   i  |   0  |   1  |   0  |  1   |   1  |      |\n|   j  |   0  |   1  |   0  |  0   |   1  |      |\n|   k  |   1  |   1  |   1  |      |      |      |\n\n So, you'll have to think of how to do this comparison differently than when we calculated allele frequencies. Think about the way to only count places where the sites match, and then similarly account for the number of individuals that have non-missing data. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_ij = function(site_i,site_j){\n    #How can we make sure that we find every case where both i and j are equal to 1, _and_ we get the number of sites correct?\n    n_ij = ???\n    n_sites =  ???\n    return(n_ij/n_sites)\n}\n```\n:::\n\n\n\nYou may be immediately tempted to run all of your code solely on the haplotypes, and that can be fine. But make sure you double check that the code is set up to work with haploid data. For instance, in the example site frequency function, we made the assumption data is diploid:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_GT = function(site){\n  num_derived = sum(site)\n  total_sites = 2*length(site)\n  return(num_derived/total_sites)\n}\n```\n:::\n\n\n\nWe need to make sure it works for any ploidy, so we might as well write a better (although less easy to read) function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq_GT = function(site,ploidy=2){\n  clean_site = na.omit(site)\n  p = sum(clean_site)/(ploidy*length(clean_site))\n  return(p)\n}\n```\n:::\n\n\n\nIn the above function, I've added a `ploidy` argument, and made the default two (so, the function will by default run on diploid data just fine). But, if you specify ploidy, it will instead use that value. In our case, we now know we'll need to run the `d_ij` function on haploid data. So, let's specify:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_ij = function(site_i,site_j){\n    p_i = freq_GT(site_i,1)\n    p_j = freq_GT(site_j,1)\n    p_ij = freq_ij(site_i,site_j)\n    D_ij = p_ij - p_i*p_j\n    return(D_ij)\n}\n```\n:::\n\n\n\n\n::: {.callout-note}\nWhat is the average $D_ij$ in the muc19 data?\n:::\n\n### $r^2$\n\nIn some ways, $r^2$ is much easier to calculate than other statistics, since it is just a correlation between haplotypes. You won't need to write a fancy new function here, in fact, you can get the whole matrix of all possible correlations with a single function call (assuming you have a haploid GT):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor_mat = cor(t(GT))\n```\n:::\n\n\n\nThis will take a while, however, and potentially crash `R` for you. That's because it's doing _a lot_ of comparisons under the hood.\n\n# Part 3: The combinatorial nightmare of LD\n\nSo far we've discussed LD as a fun and useful statistic you can calculate for your data. The problem is that it is a pairwise statistic, so the number of comparisons we need to make grows quadratically with the number of sites. It's useful to at least know about [big O notation ](https://en.wikipedia.org/wiki/Big_O_notation), but the very short version is: if you have a 1000 sites and you calculate $\\pi$ for each, that's roughly 1000 calculations, and for a 1,000,000 it's a 1,000,000. But there are $\\binom{1000}{2} = 499500$ calculations for LD with a 1000 sites, and a whopping $\\binom{1000000}{2} \\approxeq 5e11$ for 1 million sites. Modern computers are fast, but when you ask them to do on the order of trillions of calculations - your code will be very slow. You'll also need to _store_ all of the results of those calculations. At 64 bits per LD value, you'll be looking at about 4 Tb of data to store all the LD values between 1 million sites. So - how do we avoid this?\n\n###  Solution 1: Don't calculate all LD values.\n\nThis is the simplest and (often) best solution. Don't bother calculating LD between every possible pair of loci - you generally don't need to. You can vastly simplify your calculations by averaging across regions, for instance (say, pairwise LD between different genes), or by only calculating values for common alleles (require the minor allele frequency at both sites to be greater than some threshold, often 0.1). Finally, we saw in class how long-range LD tends to decay to very small values, so it may not be worth looking for LD past a certain distance. Rather than calculate `d_ij` at all pairs of sites, calculate only the ones you are deeply interested in, and if you don't know what those are - just randomly subsample a smaller subset of sites to calculate LD. Even then, this is one place where `R`s relative slow speed compared to other programming languages can still come back to haunt you. It's good to know _how_ to calculate LD yourself, but I would often recommend you don't do it.\n\nHere is an example of how you can thin the data randomly (to, say 10% of the data):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrandom_rows = sample(1:dim(GT)[1],size=round(0.1*dim(GT)[1]),replace=FALSE)\nthin_GT = GT[random_rows,]\ncor_mat = cor(t(thin_GT))\n```\n:::\n\n\n\n::: {.callout-note}\nWhat are some issues with thinning data this way?\n:::\n\n### Solution 2: Use PLINK (or some other software)\n\nThe good folks at  [Plink](https://www.cog-genomics.org/plink/) have spent a lot of time thinking about this as well, and have made their LD calculations be very nice and specific in how much data they return. The plots you saw in class are results of running `Plink` on the `muc19_subsample.vcf.gz` file, rather than running the calculations in `R`. The command to run an LD analysis is quite long, but it needs to be in order to not spit out 4Tb of data at you:\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nplink --vcf  muc19_subsample.vcf.gz \\ \n--double-id \\ #Plink expects a family, but we only have individuals. This tells it to stop that behavior (and just label each samples haplotype)\n--allow-extra-chr \\ #Plink assumes everything is a human, so if somehow the name of the chromosome is not in the human set it craps out. This stops that behavior.\n--maf 0.01 \\ #Minimum allele frequency of 0.01\n--geno 0.1 \\ #Tosses a whole site if more than 10% individuals have missing data at it\n--mind 0.5 \\ #Any individual with more than 50% missing data is tossed as well\n--chr 12 \\ #Which chromosome should it analyze\n--thin 0.1 \\ #Randomly sample 10% of loci, and use these for the analysis\n-r2 gz \\ #Output the r2 statistic, and gzip it so it's not as giant.\n--ld-window 100 \\ #Minimum distance between sites to analyze\n--ld-window-kb 1000 \\ #Maximum distance (in kb this time) of sites to analyze\n--ld-window-r2 0 \\ #Drop any sites with r2 less than 0 (so, keep all r2)\n--out your_prefix # What to call the output files\n```\n:::\n\n\n\nThis should run quite quickly on the muc19 data, even though there are quite many sites in there. Indeed, you could easily swap some parameters (say, the thin parameter) and still see decently small outputs. But, notice that the file size is nearly the same as that of the vcf even at the settings I provide above. Calculating all possible r2 values will both take the entire lab and likely fill up your hard drive.\n\nIn general, as you start working on bigger data-sets you will end up seeing exponentially longer run times. It's worth it to go back and re-think when something is actually needed, versus when you can simplify your data somewhat. In cases where you are running lots of SNP-wise stats, LD-pruning can be a very prudent approach.\n\n# Part 4: LD-pruning\n\n`Plink` can also run LD pruning, so if you've enjoyed your time with it, feel free to do generally use `Plink` for this, as it's infinitely faster than `R`. But first, let's discuss _how_ LD pruning is done in practice. In general, when you LD prune you subdivide the genome into windows of equal size. For each window, you calculate all pairwise correlations. Then, you loop through each SNP, and toss out all later SNPs in high LD with it. Below is some code that shows you the structure of how this works:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Function to LD-prune a VCF. Makes calls to subfunctions for each chromosome, and each of those runs sub-functions on each window.\n#For a variety of reasons, we drop _all_ alleles with no variation (e.g. allele frequency ==0 or 1).\n\nld_prune = function(vcf,window_size,cutoff=0.5){\n  GT = extract.haps(vcf,as.numeric=TRUE)\n  freqs = apply(GT,1,function(x) freq_GT(x,ploidy=1))\n  GT = GT[which(freqs*(1-freqs)>0),]\n  chr_id = gsub(\":.*:.*:.*\",\"\",rownames(GT))\n  chrs = sort(unique(chr_id))\n  sub_GT = sapply(chrs,function(x) ld_prune_chr(GT[chr_id==x,],window_size=window_size,cutoff=cutoff)) \n  return(sub_GT)\n}\n\n#Within each chromosome, generate windows and find only the sites to be dropped.\nld_prune_chr = function(GT,window_size,cutoff=0.5){\n  pos = as.numeric(gsub(\".*:([0-9]*):.*\",\"\\\\1\",rownames(GT)))\n  breaks = unique(c(seq(from=min(pos)-1,to=max(pos),by=window_size),max(pos)))\n  bins = cut(pos,breaks,right=TRUE)\n  #Drop bins with 1 or fewer entries\n  bad_bins = names(which(table(bins)<2))\n  bins = factor(bins,exclude=bad_bins)\n  drop = unlist(sapply(levels(bins),function(x) ld_prune_window(GT[which(bins==x),],cutoff=cutoff)))\n  sub_GT = GT[setdiff(rownames(GT),drop),]\n  return(sub_GT)\n}\n\n#Within each window, calculate correlations, keep only the first allele with high correlations\nld_prune_window = function(GT,cutoff=0.5){\n  cor_mat = cor(t(GT))\n  to_check = 1:dim(cor_mat)[1]\n  drop = c()\n  while(length(to_check)>0){\n    high_cor = which(cor_mat[to_check[1],]>cutoff)\n    drop = c(drop,setdiff(high_cor,to_check[1]))\n    to_check = setdiff(to_check,high_cor)\n  }\n  if(length(drop)>0){\n    return(rownames(GT)[drop])\n  }\n}\n```\n:::\n\n\n\n::: {.callout-note}\nWhat is the total number of SNPs that remain after you LD prune the muc19 data using a 250bp window?\n\nWhat about a 500 bp window?\n:::\n\nThe above code will work, but suffers from all of `R`s usual issues of scaling into really big data. But, by writing the different chunks as different modules, the task is infinitely parallelizable. If you don't know what that means - don't worry about it. Just realize that each window calculation can be done independently of others, so if you want to use some parallel package (like `doSnow` or `doParallel`), you can easily convert the above functions to be much, much faster.\n\n### Hasn't someone already written a faster version of this\n\nYes! As with many stats, there are many packages that can do this in `R`, but each will want a different data type. For instance, `SNPRelate` will ask for a `gdsobj`, while the `STAARS` pipeline uses an `aGDS` file format, while `bigsnpr` will use an `FBM.code256` object to ld prune. At the end of the day, you will probably decide on _some_ package that works best for you, and incorporate most of its approaches in your pipelines. But, as hard as it is managing your own code, you'll find managing interactions with many other packages becomes quite tedious and issue-prone. What if one of the formats drops NAs, while another imputes genotypes? So, again, it is good to know how to run things yourself, even if you will largely use other tools.",
    "supporting": [
      "lab4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}