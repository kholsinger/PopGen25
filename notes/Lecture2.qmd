---
title: "Lecture 2"
execute:
  freeze: auto  # re-render only when source changes
format: 
    revealjs:
        theme: solarized
        incremental: true
        preview-links: auto
        chalkboard: true
engine: julia
julia:
  exeflags: ["--project=../PopGen25"]
---

# Lecture 2: Describing genetic diversity

## Genetic diversity writ large

Last week you learned how different evolutionary forces affect allele frequencies at a single site.

. . .

This week, we'll be discussing how we can measure genetic diversity and what those measurements can tell us.

. . .

We will look at a few statistics that measure genetic diversity, notably the SFS, $\pi$, $\theta_W$ and $H$.

. . .

Before that, we'll talk about why we even care.

## What is diversity good for?

*Evolution is the change in traits of populations over generations*

. . .

Change is impossible if variation does not exist.

. . .

Genetic variation allows for evolvability - without it populations are stuck.

. . .

*Most* variation might in fact be deleterious in any particular environment.

. . .

Conversely, if the environment changes - old variants are unlikely to meet all of its needs.

## Where does diversity come from?

Ultimately, all genetic diversity begins with mutation.

. . .

A key quantity in population genetics is $\theta$ - the neutral genetic diversity.

. . .

Under Wright-Fisher: no new mutations being introduced. What is the long term expectation?

. . .

Let's first think about *how* to measure diversity.

## Measuring diversity at a site

Before we think about what's happening across the genome, we can start by asking what is happening at an individual locus.

::: callout-tip
## Recall

We will refer to the allele frequency of the *alternate* allele at site $j$ as $p$.
:::

What are some ways we can ask how diverse a population is at a site?

## Expected Heterozygosity ( *H* ) {.smaller}

One of the simplest metrics is to ask how frequently we see individuals who are heterozygous.

. . .

Let's start by generalizing to any number of alleles.

. . .

$$
H_j = 1 - \sum_{i=1}^{k}{p_i^2}
$$

Where $p_i$ is the frequency of the $i^{th}$ allele and there are a total of $k$ alleles.

. . .

If there are only two alleles:

$$1-p_a^2 -p_A^2 = 2p_a(1-p_a)$$

. . .

In practice - often not the case: why?

## Sample heterozygosity (*h*) {.smaller}

Another way to think about genetic diversity is to ask:

What is the probability that if you sample any two alleles in the population, they are different.

. . .

In an infinite population with a biallelic site, this is simply $2p(2-p)$

. . .

But populations are not infinite, and we also don't sample all possible individuals.

. . .

Instead, we calculate the expected *sample* heterozygosity, and account for number of alleles sampled ($n$):

$$
h_j=\frac{n}{n-1}\left(1-\sum_i^k{p_i^2}\right)
$$

## Heterozygosity and drift

We saw last week how drift alone leads to evolutionary change by fixation and loss of alleles due to random chance.

. . .

This can lead to a change in heterozygosity that is quite predictable.

. . .

Any generation, the given (true) heterozygosity is $H_t$. What will it be in the next generation?

. . .

We can make our lives easier by looking at the inverse $\mathscr{H}$ : the Homozygosity of the population.

. . .

Useful to know: $H = 1-\mathscr{H}$.

## How does drift change homozygosity?

Two ways for random pair of alleles to be identical:

```{dot}
digraph G {
  subgraph cluster_0 {
    node [style=filled,label=""];
    a1 [color=lightgray]
    a2 [color=lightgray]
    A1 [color=black]
    A2 [color=black]
    b1 [color=lightgray]
    b2 [color=lightgray]
    a2 -> b1 
    a2 -> b2
    label = "Identical by descent";
  }

  subgraph cluster_1 {
    node [style=filled,label=""];
    c1 [color=lightgray]
    c2 [color=lightgray]
    C1 [color=black]
    C2 [color=black]
    B1 [color=lightgray]
    B2 [color=lightgray]
    c1 -> B1 
    c2 -> B2
    label = "Identical by state";
  }
}
```

So: $\mathscr{H}_{t+1} = \frac{1}{2N} + (1-\frac{1}{2N})\mathscr{H_t}$

## Putting it together

Then:

$$
H_{t+1} = 1-\frac{1}{2N}-\left(1-\frac{1}{2N}\right)\left(1-H_t\right)=\left(1-\frac{1}{2N}\right)H_t
$$

In other words - drift causes heterozygosity to predictably decrease every generation (at a rate of $-\frac{H}{2N}$)

## Let's check simulations

We can take a look at our simulations:

```{julia}
using Plots, Measures, LaTeXStrings
theme(:solarized_light)
default(legendfontsize=18,guidefontsize=18)
include("../src/Pop_gen_code.jl")
sims = [WrightFisher(1000,0.5,1000) for _ in 1:500]
h = broadcast(x -> 2 .* x .*(1 .- x),sims)
p1 = plot(h,label="",c=:black,alpha=0.2,
xlabel="Generations",ylabel=L"H",margins=4mm,size=(800,500))
m = [mean(col) for col in eachcol(h)]
plot!(p1,m,c=:red,label="Mean",linewidth=2)
```

## Versus expectation:

```{julia}
function exp_h(h,N,t)
    hs = zeros(t)
    hs[1] = h
    for g in 1:(t-1)
        hs[g+1] = (1-1/N)*hs[g]
    end
    return(hs)
end
plot!(p1,exp_h(0.5,1000,1000),label="Theory",linewidth=2)
```

## How quickly does heterozygosity decay?

Will heterozygosity decrease faster in big or small populations? Why?

. . .

## Heterozygosity across a region

We can then zoom out and ask what the diversity is across a whole region. Say there are $L$ sites in your region, then the total *polymorphism* is given by:

$$
\pi = \sum_{j=1}^{L}{h_j}
$$

. . .

The more common definition you'll see is the *per site* *polymorphism/heterozygosity*, given as:

$$
\pi =  \frac{1}{L}\sum_{j=1}^{L}{h_j}
$$

Note, often the two will be labeled as $\pi$, but per-site is far more common.

## Let's check some intuition

Here, we'll use the example data from Hahn's textbook :

```{julia}
hahn_data = 
["T" "T" "A" "C" "A" "A" "T" "C" "C" "G" "A" "T" "C" "G" "T";
 "-" "-" "A" "C" "G" "A" "T" "G" "C" "G" "C" "T" "C" "G" "T";
 "T" "C" "A" "C" "A" "A" "T" "G" "C" "G" "A" "T" "G" "G" "A";
 "T" "T" "A" "C" "G" "A" "T" "G" "-" "-" "C" "T" "C" "G" "T"]
[join(i) for i in eachrow(hahn_data)]
```

Each row is an individual, each column is a site. Notice how some sites have missing data "-".

. . .

What is the heterozygosity at the third site?

. . .

How can we account for missing data in calculating heterozygosity?

## Code examples

```{julia}
#| echo: true
using StatsBase

function sample_h(alleles)
    obs_alleles=collect(skipmissing(alleles)) #Genotypes might be missing - we skip those.
    n=length(obs_alleles)
    freqs = values(countmap(obs_alleles)) ./ n #Get allele frequencies for each allele
    return( n/(n-1)*(1-sum(freqs .^ 2))) #Return the expected sample h
end

function sample_π(genotype_matrix)
    return(sum([sample_h(genotype_matrix[:,x]) for x in 1:size(genotype_matrix)[2]])/size(genotype_matrix)[2]) #Sum of h over all sites/number of sites
end

sample_π(replace(hahn_data,"-"=>missing))
```

## What should $\pi$ look like?

We can use some baseline Wright-Fisher model with mutation assumptions to figure out what $\pi$ will be like in practice.

. . .

Let's say there are $N$ individuals. The mutation rate is $\mu$. What is the expected heterozygosity?

## Finding steady-states

One way to solve modeling problems is to enumerate all of the forces acting on them, and find when the forces cancel each other out.

. . .

Heterozygosity can increase because of mutation.

. . .

Heterozygosity is lost because of drift (recall drift eventually causes alleles to be fixed/lost).

. . .

How strong are these forces?

## More formally {.smaller}

Let $\Delta H$ be the change in heterozygosity from one generation to the next.

Then:

$$
\Delta H = \Delta mutation + \Delta drift
$$

Any individuals *not* heterozygous can become so via mutation at either of their two copies of the locus.

$$
\Delta mutation = (1-H)2\mu
$$

And drift cause the loss of heterozygosity at the rate we derived earlier:

$\Delta drift = -\frac{H}{2N}$

Then, at steady state:

$$
0=(1-H)2\mu-\frac{H}{2N}
$$

Chalkboard time!

## Expected heterozygosity

The neutral expected heterozygosity is $4N\mu$. This is often called $\theta$.

. . .

Of course, the quantity is different for haploids: $\theta = 2N\mu$

. . .

Deviations from $\theta$ are expected to occur because of non Wright-Fisher dynamics.

. . .

But do populations actually show this much variation?

## Some examples {.smaller}

| Species | Rough *N* | Rough $\mu$ | Expected $\pi$ | More accurate | Actual $\pi$ |
|------------|------------|------------|------------|------------|------------|
| *Homo sapiens* | $8.2 * 10^9$ | $1.4 * 10^{-8}$ | 45.9 | 0.997 | 0.12-0.22 |
| *Drosophila melanogaster* | $10^{15}$ | $10^{-8}$ | $10^7$ | 0.99999999.. | 0.1-0.4 |
| *Pelagibacter ubique* | $2.4 * 10^{28}$ | $10^{-6}$ | $10^{22}$ | 1 | 0.4-0.8 |

## *Effective* population size ($N_E$)

We often refer not to the census population size, but the effective population size.

. . .

The idea is simple - not all individuals in a population actually mate and contribute to the next generation.

. . .

These individuals, from the perspective of evolutionary biology, might as well be dead/non-existant.

. . .

The *effective population size* is the size of a hypothetical population that evolves under WF dynamics.

## Implication of $N_E$

$N_E$ implies that many individuals in a population don't actually contribute to the species' evolution.

. . .

Some of this might be due to selection - individuals who simply die/don't mate.

. . .

The quantity $\frac{N_E}{N}$ should tell you what fraction of individuals are surviving/contributing.

. . .

More thoughts about this for Thursday's reading.

## Not the only heterozygosity

$\pi$ is a commonly used estimator for genetic diversity, but it has some issues. The biggest is that when true diversity is high, variance in *sample* diversity is also really high. That is, it's hard to get an accurate estimate.

```{julia}
using Plots, LaTeXStrings, Measures
function var_pi(θ,n)
    (n+1)/(3*(n-1))*θ+2(n^2+n+3)/(9*n*(n-1))*θ^2
end
ns=range(10,1000,length=50)
thetas = [var_pi.(theta,ns) for theta in [0.01,0.1,0.5,1.0]]
plot(ns,thetas,xlabel="n",ylabel=L"Var(\pi)",label=["0.01" "0.1" "0.5" "1.0"],margins=5mm)
```

## Alternatives to $\pi$ : $\theta_W$

One common alternative is to use Watterson's Theta. The idea is that we can simply look at the *number* of segregating sites $S$ (sites with at least one variant). Because as we sample more individuals, we are more likely to sample a new allele, there is a correction factor:

$$
\theta_W = \frac{S}{\sum_{i=1}^{n-1}{1/i}}
$$

```{julia}
#| echo: true
function wat_theta(gm)
    S = sum([is_segregating(i) for i in eachcol(gm)])
    a = sum([1/i for i in 1:size(gm)[1]])
    return(S/a)
end
```

## $\theta_W$ - lower variance with greater sampling

Unlike $\pi$, $\theta_W$ shows a nice decrease in sample variance with more samples.

```{julia}
function theta_var(n,theta)
    vars =sum([theta/i for i in 1:(n-1)])+sum([theta^2/(i^2) for i in 1:(n-1)])
    a = sum([1/i for i in 1:(n-1)])
    return(vars/(a^2))
end

thetas_w = [theta_var.(ns,i) for i in [0.01,0.1,0.5,1.0]]
plot(ns,thetas_w,xlabel="n",ylabel=L"Var(\theta_W)",label=["0.1" "1" "5" "10"],margins=5mm)
```

## Why is $\theta_W$ not the standard?

Accuracy from sample size is not the only potential issue:

. . .

$\theta_W$ is not robust to other sources of errors:

-   missing data

-   sequencing error

-   uneven sampling

. . .

In practice, it's often good to be able to look at multiple different estimators, and make informed conclusions about the data from them.

## Tajima's D

In fact, we'll later cover how expected differences between $\pi$ and $\theta_W$ can be used to examine population demographic history.

. . .

For now - let's keep thinking about how to summarize genetic variation in a population.

## Single data point for whole genome?

$\theta$, $\pi$, $h$ and similar are *summary statistics* - they reduce potentially huge amounts of data into a single number. That *can* be useful, but sometimes it's better to look at a denser view of the data. Let's make a larger/more complicated data-set.

```{julia}
include("../src/Pop_gen_code.jl")
include("../src/Stats_utilities.jl")
ancestral=vec(fake_alignment(1,10000))
#Just to make the PCA more exciting later - generate two "independent" pops here, one of which willl be more diverged within a middle chunk of the data.
pop1 = fake_alignment_biallelic(ancestral,50;gaps=0.01,adj=30)
pop2_1 = fake_alignment_biallelic(ancestral[1:4450],50;gaps=0.01)
pop2_2 = vcat(fake_alignment(10,500),fake_alignment_biallelic(ancestral[4451:4950],40;gaps=0,adj=50))
pop2_3 = fake_alignment_biallelic(ancestral[4951:10000],50;gaps=0.01)
pop2 = hcat(pop2_1,pop2_2,pop2_3)
geno_matrix = vcat(pop1,pop2)
[join(replace(i,missing=>"-")) for i in eachrow(geno_matrix)]
```

## Windowed statistics {.smaller}

```{julia}
function window_map(f,data,wnd_size)
    start = 1
    stop = start+wnd_size
    centers = []
    result = []
    while(stop < size(data)[2])
        append!(centers,(start+stop)/2)
        append!(result,f(data[:,start:stop]))
        start = stop
        stop = start + wnd_size
    end
    return(centers,result)
end
scatter([sample_h(i) for i in eachcol(geno_matrix)],
label="",
xlabel="Window Center",
ylabel=L"\pi",
size=(1000,500),margins=5mm,legend=:outertopright;
markersize=1,markerstrokewidth=0)
plot!(window_map(sample_π,geno_matrix,1000),label="Windowed (1000)",linewidth=1.5,linecolor=:black)
plot!(window_map(sample_π,geno_matrix,500),label="Windowed (500)",linewidth=1.5,linecolor=:red)
```

## But window choice not arbitrary

As we'll discuss in the next lecture: our choice of windows might depend on factors like recombination rate.

. . .

Frequently, studies use a *single* window value. But different parts of the genome have different degrees of LD, mutation, recombination.

. . .

In general - good idea to run a sliding window analysis.

## Sliding windows

```{julia}
function sliding_window_map(f,data,wnd_size,wnd_step;first_pos=0)
    start = 1
    stop = start+wnd_size
    centers = []
    result = []
    while(stop < size(data)[2])
        append!(centers,(start+stop)/2)
        append!(result,f(data[:,start:stop]))
        start = start + wnd_step
        stop = start + wnd_size
    end
    return(centers .+ first_pos,result)
end

xlims!(4000,6000)
ylims!(0.2,0.4)
plot!(sliding_window_map(sample_π,geno_matrix[:,3000:7000],1000,50;first_pos=3000),label="Sliding (1000,50)",linewidth=2)
plot!(sliding_window_map(sample_π,geno_matrix[:,3500:6500],500,25;first_pos=3500),label="Sliding (500,25)",linewidth=2)
plot!(sliding_window_map(sample_π,geno_matrix[:,3750:6250],250,10;first_pos=3750),label="Sliding (250,10)",linewidth=2)
```

## Longer alignment - more information

One good way to represent a whole region is to summarize the allele frequencies at each site. This is called the *Site Frequency Spectrum*, or (*Allele Frequency Spectrum*). In our complicated alignment, it helps us see the frequency of different alleles.

## SFS example:

```{julia}
#| echo: false
function SFS_folded(gm) 
    return([minimum(values(countmap(i)))/length(i) for i in eachcol(gm)]); 
end

histogram(SFS_folded(only_segregating(geno_matrix)),legend=false,xlabel="Minor Allele Frequency",ylabel="Count",margins=5mm,bins=50,size=(1000,500))
```

## Allele *polarity*: ancestral state

When you don't know what the ancestral allele is, you generally examine the *minor* allele frequency (whichever is rarer.

. . .

This results in a *folded* SFS.

. . .

But, if you have an outgroup/ancestral data, you can instead use the full SFS.

## Unfolded SFS

```{julia}
function SFS(geno_matrix)
    #This assumes the ancestral sequence is labeled as "1", diverged as greater ints.
    return([countmap(i)[1]/length(i) for i in eachcol(geno_matrix)])
end

#In this case, we'll take a random sample from pop 1 as the "ancestor"
gm_matrix = geno_mat_to_Int(geno_matrix)

histogram(SFS(only_segregating(gm_matrix)),legend=false,xlabel="Derived Allele Frequency",ylabel="Count",margins=5mm,bins=50,size=(1000,500))
```

## What do you do with the SFS

One of the uses of the SFS is, ironically, to calculate summary statistics.

. . .

Some, like $\theta_H$ estimate the genetic diversity.

. . .

Others, like Tajima's D let you estimate the influence of demography on the population.

. . .

Once we talk about demography estimation - SFS contains most data needed.

## How is variation distributed among individuals?

We can now say something about how genetic variation is sumarised across a population. *But* this gives us very little information about how the variation is distributed within each individual.

For instance, if overall $\pi$ is high - does that mean that every individual is likely to *be* heterozygous?

## Summarizing complex data: PCA {.smaller}

Principal component analysis (PCA) is an approach to take complex, highly dimensional data (like the genotypes of a bunch of individuals at very many sites), and reduce it to something simpler.

```{julia}
using MultivariateStats
x = randn(1000)
y = x+randn(1000)

pca_test_plot=scatter(x,y,leg=false,size=(900,500),xlabel="X",ylabel="Y",margins=4mm)

```

## PC1 identifies the axis of highest variability

```{julia}
pca_m = fit(PCA,transpose(hcat(x,y)))
plot!(pca_test_plot,i->i*scaled_loadings(pca_m)[1,1],-4,4)
```

## Each PC explains the next most variation {.smaller}

When data fits PCA assumptions, each subsequent PC explains the next most variance.

. . .

Technical details: the first principal component is the eigenvector of the correspondingly largest eigenvalue of a covariance matrix between data points.

. . .

i.e.: You correlate all data points, and ask what linear transformations explain the covariation. This in turn explains the most variation in the data.

. . .

## Works in genomic data too!

Here's our toy sequence data along the first two PCs.

```{julia}
gm_matrix = geno_mat_to_Int(vcat(reshape(ancestral,1,length(ancestral)),geno_matrix))
gm_PCA = fit(PCA,gm_matrix[2:101,:])
proc = round.(gm_PCA.prinvars/sum(gm_PCA.prinvars)*100;digits=2)
lab =  string.(proc)
scatter(gm_PCA.proj[:,1],gm_PCA.proj[:,2],leg=false,size=(1000,500),
    xlabel="PC 1 ("*lab[1]*" %)",ylabel="PC 2 ("*lab[2]*" %)",margins=5mm)
```

## Is PCA always worth it? {.smaller}

How do you know how many PCs are "enough" to describe your data?

. . .

PCA significance can be evaluated using the *broken stick model*.

. . .

If you choose *n* random break points of a stick of length 1, then the size of these segments follows a typical distribution:

```{julia}
scatter(reverse([broken_stick_dist(i,10) for i in 1:10]),legend=false,xlabel="PC",ylabel="Variance explained",margins=5mm,size=(800,400),xticks=(1:10))
```

## How does that compare to our toy sequence data?

```{julia}
p2 = scree(gm_PCA;broken_stick=true)
plot(p2,size=(800,600))
```

## PCA can be deceiving!

Unequal sampling can make it seem like one subgroup has all the "important" variation, but they are just sampled better.

. . .

Samples from different time points can create illusion of similarity between populations/individuals.

. . .

Interesting variation is not necessarily along the first PC axis.

. . .

In this case: 500 bp bit of basically random sequence in 10 individuals in pop2.

## Other ways of thinking about genetic variation

We've considered how much variability there is at a locus/across the genome.

. . .

We've considered how that variation might be partitioned among all the samples.

. . .

Let's ask how variation might be partitioned within a population.

## $F$-statistics (inbreeding coefficients)

We've talked a lot about *expected* heterozygosity, but actual observed heterozygosity is also informative.

. . .

Let $F$ be the coefficient of inbreeding (the probability that any two alleles are identical by descent).

. . .

Then:

$$
\begin{cases}
p_{AA} = p^2(1-F)+pF \\
p_{Aa} = 2pq(1-F) \\
p_{aa} = q^2(1-F)+qF \\
\end{cases}
$$

. . .

In other words - the frequency of heterozygotes is decreased, while homozygotes increase.

. . .

What should $F$ be in Wright-Fisher?

## $F_{IS}$ - Inbreeding statistic

Wright realized you could measure how inbred a population is by comparing the Observed Heterozygosity $H_{obs}$ with Expected Heterozygosity $H_{exp}$. For any locus, the inbreeding coefficient can be found as:

. . .

$$
F = \frac{H_{obs}}{H_{exp}}=\frac{P_{Aa}}{2pq}
$$

. . .

To actually study inbreeding it's more useful to examine this across the whole genome rather than a single locus.

## Case study

Let's look at a case study of some allele data:

| Locus          | AA   | Aa   | aa  | Total |
|----------------|------|------|-----|-------|
| Wing spotting  | 1610 | 302  | 88  | 2000  |
| Eye distortion | 1215 | 508  | 277 | 2000  |
| Jawless mouth  | 200  | 1450 | 450 | 2000  |

. . .

Should get: 0.28, 0.34, -0.4

## Are any of these *significant* deviations?

Simple to test if population is statistically deviating from HW ($\chi^2$ tests, for instance).

. . .

Rule of thumb - differences w/in 5% of expectation (e.g. $F_{IS} ~0.05$) happen in neutral simulations all of the time.

. . .

Bigger differences - actually perform testing.

## Inbred population, or inbred individuals?

One way to look for individuals who may be inbred is to look for *Runs of Homozygosity* (ROH)

. . .

We'll talk more when we cover LD and recombination, but not only should inbred individuals have more homozygous alleles than expected, these alleles should also be clustered.

# Rate of evolution

Next week we'll talk a lot more about divergence. But as a prequel, let's test our intuition again.

. . .

If populations are diverging over time, which should diverge faster: small or large populations?

. . .

## Rate of allele fixation

Recall that any new allele that's starting at frequency $\frac{1}{2N_E}$ also has a probability of $\frac{1}{2N_E}$ of becoming fixed.

. . .

Let's say there are lots of these mutations, each with probability of $\frac{1}{2N_E}$ of becoming fixed. How long do we have to wait until an allele is fixed?

. . .

If they evolve independently, this is just the `geometric` probability distribution: takes a rate parameter == chance of success.

. . .

So, on average of $2N_E$ generations for a mutation to fix, and each generation $\frac{1}{2N_E}$ mutations fix.

## But there are not infinite mutations

. . .

How many new alleles come in every generation?

. . .

There are $2N_E$ copies of the allele. Then - there are $2N_E\mu$ new mutations every generation.

## Putting it together - the basics of neutral theory

If we put all of this together, the rate of fixing mutations becomes quite predictable:

Every generation there are $2N_E\mu$ mutations coming in that are fixed at a rate of $\frac{1}{2N_E}$

The neutral evolutionary rate is therefore:

$$
\frac{2N_E\mu}{2N_E} = \mu
$$